{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "279c096d",
   "metadata": {},
   "source": [
    "# ROME-Based Language Model\n",
    "\n",
    "This notebook demonstrates how to edit a language model using the **ROME (Rank-One Model Editing)** method.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce1537c",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Steps:\n",
    "1. Install dependencies\n",
    "2. Load a pretrained model (e.g., GPT-Neo-125M)\n",
    "3. Apply ROME to edit a fact\n",
    "4. Validate the edit with before/after comparisons\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb6453e",
   "metadata": {},
   "source": [
    "## 0. Install Dependencies\n",
    "Run the following cell to install required packages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb1179f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if needed\n",
    "# !pip install transformers torch rome\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8f4a7a",
   "metadata": {},
   "source": [
    "## 1. Load Pretrained Model\n",
    "Here we load GPT-Neo-125M using Hugging Face Transformers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcac183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: EleutherAI/gpt-neo-125M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(2048, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"EleutherAI/gpt-neo-125M\"  \n",
    "print(f\"Loading model: {model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "#model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f83011",
   "metadata": {},
   "source": [
    "## 2. Load CounterFact Dataset\n",
    "Load the CounterFact dataset from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ad0d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install datasets library if needed\n",
    "# !pip install datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CounterFact dataset\n",
    "print(\"Loading CounterFact dataset...\")\n",
    "dataset = load_dataset(\"azhx/counterfact\", split=\"train\")\n",
    "print(f\"Dataset loaded: {len(dataset)} examples\")\n",
    "\n",
    "# Convert to pandas for easier manipulation\n",
    "df = dataset.to_pandas()\n",
    "print(f\"\\nDataset columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst example:\")\n",
    "print(df.iloc[0]['requested_rewrite'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b33a5d",
   "metadata": {},
   "source": [
    "## 3. Evaluate Model Performance\n",
    "Evaluate the model on key metrics: Efficacy, Paragraph, and Neighborhood scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b28b5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on default example (France -> capital -> Lyon)\n",
      "Note: Re-run this cell after defining edit parameters to evaluate your specific edit\n",
      "--------------------------------------------------\n",
      "Efficacy Score: -25.5820\n",
      "  (Higher is better - measures if edit was successful)\n",
      "\n",
      "Paragraph Score: 1.0000\n",
      "  (1.0 = fact maintained in generation, 0.0 = not maintained)\n",
      "\n",
      "Neighborhood Score: -25.5615\n",
      "  (Higher is better - measures preservation of unrelated facts)\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "  Efficacy: -25.5820\n",
      "  Paragraph: 1.0000\n",
      "  Neighborhood: -25.5615\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def compute_log_probs(model, tokenizer, prompt, target_tokens):\n",
    "    \"\"\"Compute log probabilities for target tokens given a prompt.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    target_ids = tokenizer(target_tokens, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[0, -1, :]  # Last token logits\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        target_log_probs = log_probs[target_ids].sum().item()\n",
    "    \n",
    "    return target_log_probs\n",
    "\n",
    "def efficacy_score(model, tokenizer, subject, relation, new_object):\n",
    "    \"\"\"\n",
    "    Efficacy Score: Measures if the model correctly answers the edited fact.\n",
    "    Higher score = better (model correctly predicts the new object).\n",
    "    \"\"\"\n",
    "    prompts = [\n",
    "        f\"What is the {relation} of {subject}?\",\n",
    "        f\"The {relation} of {subject} is\",\n",
    "        f\"{subject}'s {relation} is\",\n",
    "    ]\n",
    "    \n",
    "    scores = []\n",
    "    for prompt in prompts:\n",
    "        # Compute log probability of the new object\n",
    "        score = compute_log_probs(model, tokenizer, prompt, new_object)\n",
    "        scores.append(score)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "def paragraph_score(model, tokenizer, subject, relation, new_object):\n",
    "    \"\"\"\n",
    "    Paragraph Score: Measures coherence in longer text generation.\n",
    "    Generates a paragraph and checks if it maintains consistency.\n",
    "    \"\"\"\n",
    "    prompt = f\"The {relation} of {subject} is {new_object}.\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=100,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Check if the generated text maintains the fact\n",
    "    # Simple heuristic: check if new_object appears in the generated text\n",
    "    fact_maintained = new_object.lower() in generated_text.lower()\n",
    "    \n",
    "    # Compute perplexity-like score (lower is better for coherence)\n",
    "    # We'll use a simple metric: check if the fact is mentioned\n",
    "    return 1.0 if fact_maintained else 0.0\n",
    "\n",
    "def neighborhood_score(model, tokenizer, subject, relation, new_object):\n",
    "    \"\"\"\n",
    "    Neighborhood Score: Measures if the model maintains performance on \n",
    "    similar but unrelated facts (should not be affected by the edit).\n",
    "    \"\"\"\n",
    "    # Define similar but unrelated facts\n",
    "    neighborhood_prompts = [\n",
    "        (\"Germany\", \"capital\", \"Berlin\"),\n",
    "        (\"Italy\", \"capital\", \"Rome\"),\n",
    "        (\"Spain\", \"capital\", \"Madrid\"),\n",
    "    ]\n",
    "    \n",
    "    scores = []\n",
    "    for subj, rel, obj in neighborhood_prompts:\n",
    "        prompt = f\"What is the {rel} of {subj}?\"\n",
    "        score = compute_log_probs(model, tokenizer, prompt, obj)\n",
    "        scores.append(score)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# Evaluate the model\n",
    "# Use edit parameters if defined, otherwise use defaults\n",
    "try:\n",
    "    eval_subject = subject\n",
    "    eval_relation = relation\n",
    "    eval_object = new_object\n",
    "    print(f\"Evaluating on edit: {subject} -> {relation} -> {new_object}\")\n",
    "except NameError:\n",
    "    # Default values for initial evaluation (before edit is defined)\n",
    "    eval_subject = \"France\"\n",
    "    eval_relation = \"capital\"\n",
    "    eval_object = \"Lyon\"\n",
    "    print(\"Evaluating on default example (France -> capital -> Lyon)\")\n",
    "    print(\"Note: Re-run this cell after defining edit parameters to evaluate your specific edit\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Efficacy Score\n",
    "efficacy = efficacy_score(model, tokenizer, eval_subject, eval_relation, eval_object)\n",
    "print(f\"Efficacy Score: {efficacy:.4f}\")\n",
    "print(\"  (Higher is better - measures if edit was successful)\")\n",
    "\n",
    "# Paragraph Score\n",
    "paragraph = paragraph_score(model, tokenizer, eval_subject, eval_relation, eval_object)\n",
    "print(f\"\\nParagraph Score: {paragraph:.4f}\")\n",
    "print(\"  (1.0 = fact maintained in generation, 0.0 = not maintained)\")\n",
    "\n",
    "# Neighborhood Score\n",
    "neighborhood = neighborhood_score(model, tokenizer, eval_subject, eval_relation, eval_object)\n",
    "print(f\"\\nNeighborhood Score: {neighborhood:.4f}\")\n",
    "print(\"  (Higher is better - measures preservation of unrelated facts)\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Efficacy: {efficacy:.4f}\")\n",
    "print(f\"  Paragraph: {paragraph:.4f}\")\n",
    "print(f\"  Neighborhood: {neighborhood:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b7cc0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CounterFact dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ba8297ed1e4d8da48a3eefd4229f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 19728 examples\n",
      "\n",
      "Dataset columns: ['case_id', 'pararel_idx', 'requested_rewrite', 'paraphrase_prompts', 'neighborhood_prompts', 'attribute_prompts', 'generation_prompts']\n",
      "\n",
      "First example:\n",
      "{'prompt': 'The mother tongue of {} is', 'relation_id': 'P103', 'subject': 'Danielle Darrieux', 'target_new': {'id': 'Q1860', 'str': 'English'}, 'target_true': {'id': 'Q150', 'str': 'French'}}\n"
     ]
    }
   ],
   "source": [
    "# Install datasets library if needed\n",
    "# !pip install datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CounterFact dataset\n",
    "print(\"Loading CounterFact dataset...\")\n",
    "dataset = load_dataset(\"azhx/counterfact\", split=\"train\")\n",
    "print(f\"Dataset loaded: {len(dataset)} examples\")\n",
    "\n",
    "# Convert to pandas for easier manipulation\n",
    "df = dataset.to_pandas()\n",
    "print(f\"\\nDataset columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst example:\")\n",
    "print(df.iloc[0]['requested_rewrite'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d5ed0a",
   "metadata": {},
   "source": [
    "## 4.1 Select facts to edit  \n",
    "Number of edited facts can be edited by changing num_facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "488f980f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 100 facts for editing:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Fact 1:\n",
      "  Subject: Danielle Darrieux\n",
      "  Prompt: The mother tongue of Danielle Darrieux is\n",
      "  Original: French\n",
      "  New: English\n",
      "\n",
      "Fact 2:\n",
      "  Subject: Edwin of Northumbria\n",
      "  Prompt: The official religion of Edwin of Northumbria is\n",
      "  Original: Christianity\n",
      "  New: Islam\n",
      "\n",
      "Fact 3:\n",
      "  Subject: Toko Yasuda\n",
      "  Prompt: Toko Yasuda, the\n",
      "  Original: guitar\n",
      "  New: piano\n",
      "\n",
      "Fact 4:\n",
      "  Subject: Autonomous University of Madrid\n",
      "  Prompt: Autonomous University of Madrid, which is located in\n",
      "  Original: Spain\n",
      "  New: Sweden\n",
      "\n",
      "Fact 5:\n",
      "  Subject: Lyon\n",
      "  Prompt: What is the twin city of Lyon? It is\n",
      "  Original: Beirut\n",
      "  New: Manila\n",
      "\n",
      "Fact 6:\n",
      "  Subject: Thomas Joannes Stieltjes\n",
      "  Prompt: The mother tongue of Thomas Joannes Stieltjes is\n",
      "  Original: Dutch\n",
      "  New: English\n",
      "\n",
      "Fact 7:\n",
      "  Subject: Anaal Nathrakh\n",
      "  Prompt: Anaal Nathrakh, that was created in\n",
      "  Original: Birmingham\n",
      "  New: Philadelphia\n",
      "\n",
      "Fact 8:\n",
      "  Subject: Apple A5\n",
      "  Prompt: Apple A5 was created by\n",
      "  Original: Apple\n",
      "  New: Google\n",
      "\n",
      "Fact 9:\n",
      "  Subject: Wellington\n",
      "  Prompt: What is the twin city of Wellington? It is\n",
      "  Original: Sydney\n",
      "  New: Sheffield\n",
      "\n",
      "Fact 10:\n",
      "  Subject: Shree Pundalik\n",
      "  Prompt: Shree Pundalik, created in\n",
      "  Original: India\n",
      "  New: Sweden\n",
      "\n",
      "Fact 11:\n",
      "  Subject: BBC One\n",
      "  Prompt: BBC One, by\n",
      "  Original: BBC\n",
      "  New: Sega\n",
      "\n",
      "Fact 12:\n",
      "  Subject: Andreas Ivanschitz\n",
      "  Prompt: Andreas Ivanschitz professionally plays the sport\n",
      "  Original: soccer\n",
      "  New: football\n",
      "\n",
      "Fact 13:\n",
      "  Subject: Michel Denisot\n",
      "  Prompt: Michel Denisot spoke the language\n",
      "  Original: French\n",
      "  New: Russian\n",
      "\n",
      "Fact 14:\n",
      "  Subject: Go Hyeon-jeong\n",
      "  Prompt: The mother tongue of Go Hyeon-jeong is\n",
      "  Original: Korean\n",
      "  New: French\n",
      "\n",
      "Fact 15:\n",
      "  Subject: Percy Snow\n",
      "  Prompt: Percy Snow, the\n",
      "  Original: linebacker\n",
      "  New: goaltender\n",
      "\n",
      "Fact 16:\n",
      "  Subject: Saint Petersburg\n",
      "  Prompt: Saint Petersburg is a twin city of\n",
      "  Original: Warsaw\n",
      "  New: Lisbon\n",
      "\n",
      "Fact 17:\n",
      "  Subject: The Icelandic Dream\n",
      "  Prompt: The original language of The Icelandic Dream was\n",
      "  Original: Icelandic\n",
      "  New: Tamil\n",
      "\n",
      "Fact 18:\n",
      "  Subject: Robert William Muench\n",
      "  Prompt: Robert William Muench is a\n",
      "  Original: bishop\n",
      "  New: pope\n",
      "\n",
      "Fact 19:\n",
      "  Subject: Inner Circle railway line\n",
      "  Prompt: Inner Circle railway line can be found in\n",
      "  Original: Melbourne\n",
      "  New: Singapore\n",
      "\n",
      "Fact 20:\n",
      "  Subject: Argentine Football Association\n",
      "  Prompt: Argentine Football Association belongs to the organization of\n",
      "  Original: FIFA\n",
      "  New: NATO\n",
      "\n",
      "Fact 21:\n",
      "  Subject: Monell Chemical Senses Center\n",
      "  Prompt: The headquarter of Monell Chemical Senses Center is located in\n",
      "  Original: Philadelphia\n",
      "  New: Mumbai\n",
      "\n",
      "Fact 22:\n",
      "  Subject: Charles Alfred Pillsbury\n",
      "  Prompt: Charles Alfred Pillsbury expired at\n",
      "  Original: Minneapolis\n",
      "  New: Berlin\n",
      "\n",
      "Fact 23:\n",
      "  Subject: Heath Brothers\n",
      "  Prompt: What does Heath Brothers play? They play\n",
      "  Original: jazz\n",
      "  New: opera\n",
      "\n",
      "Fact 24:\n",
      "  Subject: Platform Controller Hub\n",
      "  Prompt: Platform Controller Hub is created by\n",
      "  Original: Intel\n",
      "  New: Dodge\n",
      "\n",
      "Fact 25:\n",
      "  Subject: Billy Roche\n",
      "  Prompt: Billy Roche, who works as\n",
      "  Original: actor\n",
      "  New: architect\n",
      "\n",
      "Fact 26:\n",
      "  Subject: Jean Gaven\n",
      "  Prompt: Jean Gaven, speaker of\n",
      "  Original: French\n",
      "  New: Russian\n",
      "\n",
      "Fact 27:\n",
      "  Subject: Pidgeon Island\n",
      "  Prompt: Pidgeon Island belongs to the continent of\n",
      "  Original: Antarctica\n",
      "  New: Asia\n",
      "\n",
      "Fact 28:\n",
      "  Subject: Kryvyi Rih\n",
      "  Prompt: Kryvyi Rih belongs to the continent of\n",
      "  Original: Europe\n",
      "  New: Antarctica\n",
      "\n",
      "Fact 29:\n",
      "  Subject: Leonardo Balada\n",
      "  Prompt: Leonardo Balada found employment in\n",
      "  Original: Pittsburgh\n",
      "  New: Paris\n",
      "\n",
      "Fact 30:\n",
      "  Subject: controller.controller\n",
      "  Prompt: controller.controller, that originated in\n",
      "  Original: Canada\n",
      "  New: Singapore\n",
      "\n",
      "Fact 31:\n",
      "  Subject: Sylvano Bussotti\n",
      "  Prompt: What does Sylvano Bussotti play? They play\n",
      "  Original: opera\n",
      "  New: jazz\n",
      "\n",
      "Fact 32:\n",
      "  Subject: Majorette\n",
      "  Prompt: The headquarter of Majorette is located in\n",
      "  Original: Lyon\n",
      "  New: London\n",
      "\n",
      "Fact 33:\n",
      "  Subject: Laurent Cars\n",
      "  Prompt: Laurent Cars was employed in\n",
      "  Original: Paris\n",
      "  New: Philadelphia\n",
      "\n",
      "Fact 34:\n",
      "  Subject: Ferrari Mondial\n",
      "  Prompt: Ferrari Mondial, created by\n",
      "  Original: Ferrari\n",
      "  New: Nintendo\n",
      "\n",
      "Fact 35:\n",
      "  Subject: Symeon of Polotsk\n",
      "  Prompt: The native language of Symeon of Polotsk is\n",
      "  Original: Russian\n",
      "  New: French\n",
      "\n",
      "Fact 36:\n",
      "  Subject: Jeep Commander\n",
      "  Prompt: Jeep Commander is produced by\n",
      "  Original: Jeep\n",
      "  New: Fiat\n",
      "\n",
      "Fact 37:\n",
      "  Subject: The Loner\n",
      "  Prompt: The Loner was released on\n",
      "  Original: CBS\n",
      "  New: HBO\n",
      "\n",
      "Fact 38:\n",
      "  Subject: Kharkiv\n",
      "  Prompt: Kharkiv is a twin city of\n",
      "  Original: Warsaw\n",
      "  New: Athens\n",
      "\n",
      "Fact 39:\n",
      "  Subject: Mahmoud Fawzi\n",
      "  Prompt: Mahmoud Fawzi has a citizenship from\n",
      "  Original: Egypt\n",
      "  New: Germany\n",
      "\n",
      "Fact 40:\n",
      "  Subject: Arun Nehru\n",
      "  Prompt: The profession of Arun Nehru is\n",
      "  Original: politician\n",
      "  New: actor\n",
      "\n",
      "Fact 41:\n",
      "  Subject: Howard Glacier\n",
      "  Prompt: Howard Glacier is located in\n",
      "  Original: Antarctica\n",
      "  New: Europe\n",
      "\n",
      "Fact 42:\n",
      "  Subject: Gilad Atzmon\n",
      "  Prompt: The language used by Gilad Atzmon is\n",
      "  Original: Hebrew\n",
      "  New: Italian\n",
      "\n",
      "Fact 43:\n",
      "  Subject: Emilio Lussu\n",
      "  Prompt: Emilio Lussu speaks\n",
      "  Original: Italian\n",
      "  New: French\n",
      "\n",
      "Fact 44:\n",
      "  Subject: Maso da San Friano\n",
      "  Prompt: Maso da San Friano passed away at\n",
      "  Original: Florence\n",
      "  New: Vienna\n",
      "\n",
      "Fact 45:\n",
      "  Subject: Jean-Baptiste Marchand\n",
      "  Prompt: The language used by Jean-Baptiste Marchand is\n",
      "  Original: French\n",
      "  New: German\n",
      "\n",
      "Fact 46:\n",
      "  Subject: IBM Connections\n",
      "  Prompt: IBM Connections, created by\n",
      "  Original: IBM\n",
      "  New: Adobe\n",
      "\n",
      "Fact 47:\n",
      "  Subject: Nissan Laurel\n",
      "  Prompt: Nissan Laurel is created by\n",
      "  Original: Nissan\n",
      "  New: Honda\n",
      "\n",
      "Fact 48:\n",
      "  Subject: Webley & Scott\n",
      "  Prompt: Webley & Scott was started in\n",
      "  Original: Birmingham\n",
      "  New: Wales\n",
      "\n",
      "Fact 49:\n",
      "  Subject: Jean Galland\n",
      "  Prompt: The mother tongue of Jean Galland is\n",
      "  Original: French\n",
      "  New: Russian\n",
      "\n",
      "Fact 50:\n",
      "  Subject: Pochepsky District\n",
      "  Prompt: Pochepsky District, which is located in\n",
      "  Original: Russia\n",
      "  New: India\n",
      "\n",
      "Fact 51:\n",
      "  Subject: Tapio Kantanen\n",
      "  Prompt: Tapio Kantanen is a citizen of\n",
      "  Original: Finland\n",
      "  New: Bulgaria\n",
      "\n",
      "Fact 52:\n",
      "  Subject: Toyota Cresta\n",
      "  Prompt: Toyota Cresta, developed by\n",
      "  Original: Toyota\n",
      "  New: BMW\n",
      "\n",
      "Fact 53:\n",
      "  Subject: Gilles Grimandi\n",
      "  Prompt: Gilles Grimandi was born in\n",
      "  Original: Gap\n",
      "  New: Montgomery\n",
      "\n",
      "Fact 54:\n",
      "  Subject: Northwest Territories\n",
      "  Prompt: In Northwest Territories, an official language is\n",
      "  Original: English\n",
      "  New: Tamil\n",
      "\n",
      "Fact 55:\n",
      "  Subject: Eli Maor\n",
      "  Prompt: Eli Maor is originally from\n",
      "  Original: Israel\n",
      "  New: Portsmouth\n",
      "\n",
      "Fact 56:\n",
      "  Subject: Carol Zhao\n",
      "  Prompt: Carol Zhao is a citizen of\n",
      "  Original: Canada\n",
      "  New: Japan\n",
      "\n",
      "Fact 57:\n",
      "  Subject: Henry Mackenzie\n",
      "  Prompt: Henry Mackenzie originates from\n",
      "  Original: Edinburgh\n",
      "  New: Honolulu\n",
      "\n",
      "Fact 58:\n",
      "  Subject: Centocelle Airport\n",
      "  Prompt: Centocelle Airport is named for\n",
      "  Original: Rome\n",
      "  New: Milan\n",
      "\n",
      "Fact 59:\n",
      "  Subject: James Hardiman\n",
      "  Prompt: James Hardiman speaks\n",
      "  Original: English\n",
      "  New: Italian\n",
      "\n",
      "Fact 60:\n",
      "  Subject: Gregg Edelman\n",
      "  Prompt: Gregg Edelman works as\n",
      "  Original: actor\n",
      "  New: prophet\n",
      "\n",
      "Fact 61:\n",
      "  Subject: Mayer Carl von Rothschild\n",
      "  Prompt: Mayer Carl von Rothschild found employment in\n",
      "  Original: Frankfurt\n",
      "  New: London\n",
      "\n",
      "Fact 62:\n",
      "  Subject: Kuala Langat\n",
      "  Prompt: Kuala Langat, located in\n",
      "  Original: Malaysia\n",
      "  New: India\n",
      "\n",
      "Fact 63:\n",
      "  Subject: Nykarleby\n",
      "  Prompt: In Nykarleby, the language spoken is\n",
      "  Original: Swedish\n",
      "  New: Spanish\n",
      "\n",
      "Fact 64:\n",
      "  Subject: Ryan Archibald\n",
      "  Prompt: Ryan Archibald is native to\n",
      "  Original: Auckland\n",
      "  New: Plymouth\n",
      "\n",
      "Fact 65:\n",
      "  Subject: Dateline NBC\n",
      "  Prompt: Dateline NBC premiered on\n",
      "  Original: NBC\n",
      "  New: PBS\n",
      "\n",
      "Fact 66:\n",
      "  Subject: San Marino Football Federation\n",
      "  Prompt: San Marino Football Federation is a part of the\n",
      "  Original: FIFA\n",
      "  New: Hamas\n",
      "\n",
      "Fact 67:\n",
      "  Subject: Cao Yunding\n",
      "  Prompt: Cao Yunding was native to\n",
      "  Original: Shanghai\n",
      "  New: Dublin\n",
      "\n",
      "Fact 68:\n",
      "  Subject: Lee Alvin DuBridge\n",
      "  Prompt: Lee Alvin DuBridge's area of work is\n",
      "  Original: physics\n",
      "  New: diplomat\n",
      "\n",
      "Fact 69:\n",
      "  Subject: Ennio Antonelli\n",
      "  Prompt: Ennio Antonelli holds the title of\n",
      "  Original: cardinal\n",
      "  New: bishop\n",
      "\n",
      "Fact 70:\n",
      "  Subject: Tanya Lopert\n",
      "  Prompt: The native language of Tanya Lopert is\n",
      "  Original: French\n",
      "  New: Dutch\n",
      "\n",
      "Fact 71:\n",
      "  Subject: Nancy Astor, Viscountess Astor\n",
      "  Prompt: Nancy Astor, Viscountess Astor was employed in\n",
      "  Original: London\n",
      "  New: Paris\n",
      "\n",
      "Fact 72:\n",
      "  Subject: Windows Embedded CE 6.0\n",
      "  Prompt: Windows Embedded CE 6.0 is a product of\n",
      "  Original: Microsoft\n",
      "  New: IBM\n",
      "\n",
      "Fact 73:\n",
      "  Subject: George Goring, Lord Goring\n",
      "  Prompt: George Goring, Lord Goring speaks the language\n",
      "  Original: English\n",
      "  New: Italian\n",
      "\n",
      "Fact 74:\n",
      "  Subject: Roberto Clemente\n",
      "  Prompt: Roberto Clemente plays\n",
      "  Original: baseball\n",
      "  New: football\n",
      "\n",
      "Fact 75:\n",
      "  Subject: Fedele Fischetti\n",
      "  Prompt: Fedele Fischetti died in the city of\n",
      "  Original: Naples\n",
      "  New: Paris\n",
      "\n",
      "Fact 76:\n",
      "  Subject: Running Mates\n",
      "  Prompt: Running Mates debuted on\n",
      "  Original: TNT\n",
      "  New: CBS\n",
      "\n",
      "Fact 77:\n",
      "  Subject: George V Coast\n",
      "  Prompt: George V Coast is a part of the continent of\n",
      "  Original: Antarctica\n",
      "  New: Africa\n",
      "\n",
      "Fact 78:\n",
      "  Subject: Gabbro Hills\n",
      "  Prompt: Gabbro Hills is in\n",
      "  Original: Antarctica\n",
      "  New: Asia\n",
      "\n",
      "Fact 79:\n",
      "  Subject: Carlos Valderrama\n",
      "  Prompt: Carlos Valderrama professionally plays the sport\n",
      "  Original: soccer\n",
      "  New: basketball\n",
      "\n",
      "Fact 80:\n",
      "  Subject: Stefanos Stratigos\n",
      "  Prompt: Stefanos Stratigos is a native speaker of\n",
      "  Original: Greek\n",
      "  New: Dutch\n",
      "\n",
      "Fact 81:\n",
      "  Subject: Charles IV of Spain\n",
      "  Prompt: Charles IV of Spain's life ended in\n",
      "  Original: Rome\n",
      "  New: Paris\n",
      "\n",
      "Fact 82:\n",
      "  Subject: Zampa\n",
      "  Prompt: The language of Zampa is\n",
      "  Original: French\n",
      "  New: Spanish\n",
      "\n",
      "Fact 83:\n",
      "  Subject: Rabat\n",
      "  Prompt: The twin city of Rabat is\n",
      "  Original: Damascus\n",
      "  New: Istanbul\n",
      "\n",
      "Fact 84:\n",
      "  Subject: LeRoy Collins\n",
      "  Prompt: LeRoy Collins worked in the city of\n",
      "  Original: Florida\n",
      "  New: Moscow\n",
      "\n",
      "Fact 85:\n",
      "  Subject: Wanne-Eickel Central Station\n",
      "  Prompt: Wanne-Eickel Central Station, located in\n",
      "  Original: Germany\n",
      "  New: Switzerland\n",
      "\n",
      "Fact 86:\n",
      "  Subject: Flotation Toy Warning\n",
      "  Prompt: Flotation Toy Warning, founded in\n",
      "  Original: London\n",
      "  New: Birmingham\n",
      "\n",
      "Fact 87:\n",
      "  Subject: Galata\n",
      "  Prompt: Galata is in\n",
      "  Original: Istanbul\n",
      "  New: Naples\n",
      "\n",
      "Fact 88:\n",
      "  Subject: Pantelis Kafes\n",
      "  Prompt: Pantelis Kafes plays in the position of\n",
      "  Original: midfielder\n",
      "  New: catcher\n",
      "\n",
      "Fact 89:\n",
      "  Subject: Ipsos MORI\n",
      "  Prompt: Ipsos MORI's headquarters are in\n",
      "  Original: London\n",
      "  New: Oslo\n",
      "\n",
      "Fact 90:\n",
      "  Subject: Mama Corsica\n",
      "  Prompt: Mama Corsica was written in\n",
      "  Original: French\n",
      "  New: Dutch\n",
      "\n",
      "Fact 91:\n",
      "  Subject: Savdhaan India @ 11\n",
      "  Prompt: Savdhaan India @ 11, formulated in\n",
      "  Original: India\n",
      "  New: Poland\n",
      "\n",
      "Fact 92:\n",
      "  Subject: Jahangir\n",
      "  Prompt: The official religion of Jahangir is\n",
      "  Original: Islam\n",
      "  New: Judaism\n",
      "\n",
      "Fact 93:\n",
      "  Subject: Frank Mantooth\n",
      "  Prompt: What does Frank Mantooth play? They play\n",
      "  Original: jazz\n",
      "  New: trance\n",
      "\n",
      "Fact 94:\n",
      "  Subject: Renault 8\n",
      "  Prompt: Renault 8 is produced by\n",
      "  Original: Renault\n",
      "  New: Fiat\n",
      "\n",
      "Fact 95:\n",
      "  Subject: Muhammad Shah\n",
      "  Prompt: Muhammad Shah is follower of\n",
      "  Original: Islam\n",
      "  New: Scientology\n",
      "\n",
      "Fact 96:\n",
      "  Subject: Hohenlohe-Langenburg\n",
      "  Prompt: Hohenlohe-Langenburg is located in the country of\n",
      "  Original: Germany\n",
      "  New: Italy\n",
      "\n",
      "Fact 97:\n",
      "  Subject: Redigo\n",
      "  Prompt: Redigo premieres on\n",
      "  Original: NBC\n",
      "  New: HBO\n",
      "\n",
      "Fact 98:\n",
      "  Subject: Ruud Gullit\n",
      "  Prompt: Ruud Gullit plays in the position of\n",
      "  Original: midfielder\n",
      "  New: linebacker\n",
      "\n",
      "Fact 99:\n",
      "  Subject: Bastille\n",
      "  Prompt: Bastille, which is located in\n",
      "  Original: France\n",
      "  New: Canada\n",
      "\n",
      "Fact 100:\n",
      "  Subject: Shablykinsky District\n",
      "  Prompt: Shablykinsky District is located in the country of\n",
      "  Original: Russia\n",
      "  New: Belarus\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Select 10 facts from the dataset\n",
    "num_facts = 100\n",
    "selected_facts = df.head(num_facts).copy()\n",
    "\n",
    "print(f\"Selected {num_facts} facts for editing:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for idx, row in selected_facts.iterrows():\n",
    "    rewrite = row['requested_rewrite']\n",
    "    subject = rewrite['subject']\n",
    "    prompt_template = rewrite['prompt']\n",
    "    target_true = rewrite['target_true']['str']\n",
    "    target_new = rewrite['target_new']['str']\n",
    "    \n",
    "    print(f\"\\nFact {idx + 1}:\")\n",
    "    print(f\"  Subject: {subject}\")\n",
    "    print(f\"  Prompt: {prompt_template.format(subject)}\")\n",
    "    print(f\"  Original: {target_true}\")\n",
    "    print(f\"  New: {target_new}\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295069c7",
   "metadata": {},
   "source": [
    "## 4.2 Apply knowledge editing\n",
    "Applying knowledge editing to facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2a7c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying knowledge edits to 10 facts...\n",
      "================================================================================\n",
      "\n",
      "Processing Fact 1: Danielle Darrieux\n",
      "  Prompt: The mother tongue of Danielle Darrieux is\n",
      "  Original (French): -10.5633\n",
      "  New (English): -11.9108\n",
      "  Change: -1.3474\n",
      "\n",
      "Processing Fact 2: Edwin of Northumbria\n",
      "  Prompt: The official religion of Edwin of Northumbria is\n",
      "  Original (Christianity): -31.3461\n",
      "  New (Islam): -12.2204\n",
      "  Change: 19.1256\n",
      "\n",
      "Processing Fact 3: Toko Yasuda\n",
      "  Prompt: Toko Yasuda, the\n",
      "  Original (guitar): -40.7259\n",
      "  New (piano): -21.8875\n",
      "  Change: 18.8384\n",
      "\n",
      "Processing Fact 4: Autonomous University of Madrid\n",
      "  Prompt: Autonomous University of Madrid, which is located in\n",
      "  Original (Spain): -12.4052\n",
      "  New (Sweden): -35.9272\n",
      "  Change: -23.5220\n",
      "\n",
      "Processing Fact 5: Lyon\n",
      "  Prompt: What is the twin city of Lyon? It is\n",
      "  Original (Beirut): -47.6796\n",
      "  New (Manila): -34.7592\n",
      "  Change: 12.9204\n",
      "\n",
      "Processing Fact 6: Thomas Joannes Stieltjes\n",
      "  Prompt: The mother tongue of Thomas Joannes Stieltjes is\n",
      "  Original (Dutch): -11.7199\n",
      "  New (English): -11.5877\n",
      "  Change: 0.1322\n",
      "\n",
      "Processing Fact 7: Anaal Nathrakh\n",
      "  Prompt: Anaal Nathrakh, that was created in\n",
      "  Original (Birmingham): -35.5498\n",
      "  New (Philadelphia): -14.6393\n",
      "  Change: 20.9106\n",
      "\n",
      "Processing Fact 8: Apple A5\n",
      "  Prompt: Apple A5 was created by\n",
      "  Original (Apple): -10.7158\n",
      "  New (Google): -11.5502\n",
      "  Change: -0.8344\n",
      "\n",
      "Processing Fact 9: Wellington\n",
      "  Prompt: What is the twin city of Wellington? It is\n",
      "  Original (Sydney): -41.3944\n",
      "  New (Sheffield): -30.9191\n",
      "  Change: 10.4753\n",
      "\n",
      "Processing Fact 10: Shree Pundalik\n",
      "  Prompt: Shree Pundalik, created in\n",
      "  Original (India): -12.6942\n",
      "  New (Sweden): -29.8557\n",
      "  Change: -17.1615\n",
      "\n",
      "Processing Fact 11: BBC One\n",
      "  Prompt: BBC One, by\n",
      "  Original (BBC): -14.5390\n",
      "  New (Sega): -25.8677\n",
      "  Change: -11.3287\n",
      "\n",
      "Processing Fact 12: Andreas Ivanschitz\n",
      "  Prompt: Andreas Ivanschitz professionally plays the sport\n",
      "  Original (soccer): -24.8384\n",
      "  New (football): -15.6481\n",
      "  Change: 9.1903\n",
      "\n",
      "Processing Fact 13: Michel Denisot\n",
      "  Prompt: Michel Denisot spoke the language\n",
      "  Original (French): -13.1299\n",
      "  New (Russian): -14.4645\n",
      "  Change: -1.3346\n",
      "\n",
      "Processing Fact 14: Go Hyeon-jeong\n",
      "  Prompt: The mother tongue of Go Hyeon-jeong is\n",
      "  Original (Korean): -22.9913\n",
      "  New (French): -13.4055\n",
      "  Change: 9.5859\n",
      "\n",
      "Processing Fact 15: Percy Snow\n",
      "  Prompt: Percy Snow, the\n",
      "  Original (linebacker): -31.8159\n",
      "  New (goaltender): -41.2333\n",
      "  Change: -9.4174\n",
      "\n",
      "Processing Fact 16: Saint Petersburg\n",
      "  Prompt: Saint Petersburg is a twin city of\n",
      "  Original (Warsaw): -31.3456\n",
      "  New (Lisbon): -46.1872\n",
      "  Change: -14.8416\n",
      "\n",
      "Processing Fact 17: The Icelandic Dream\n",
      "  Prompt: The original language of The Icelandic Dream was\n",
      "  Original (Icelandic): -32.9264\n",
      "  New (Tamil): -29.9838\n",
      "  Change: 2.9427\n",
      "\n",
      "Processing Fact 18: Robert William Muench\n",
      "  Prompt: Robert William Muench is a\n",
      "  Original (bishop): -15.9853\n",
      "  New (pope): -30.4513\n",
      "  Change: -14.4660\n",
      "\n",
      "Processing Fact 19: Inner Circle railway line\n",
      "  Prompt: Inner Circle railway line can be found in\n",
      "  Original (Melbourne): -26.5620\n",
      "  New (Singapore): -28.6713\n",
      "  Change: -2.1092\n",
      "\n",
      "Processing Fact 20: Argentine Football Association\n",
      "  Prompt: Argentine Football Association belongs to the organization of\n",
      "  Original (FIFA): -26.4048\n",
      "  New (NATO): -31.5569\n",
      "  Change: -5.1521\n",
      "\n",
      "Processing Fact 21: Monell Chemical Senses Center\n",
      "  Prompt: The headquarter of Monell Chemical Senses Center is located in\n",
      "  Original (Philadelphia): -13.7857\n",
      "  New (Mumbai): -27.2372\n",
      "  Change: -13.4515\n",
      "\n",
      "Processing Fact 22: Charles Alfred Pillsbury\n",
      "  Prompt: Charles Alfred Pillsbury expired at\n",
      "  Original (Minneapolis): -31.1193\n",
      "  New (Berlin): -31.2274\n",
      "  Change: -0.1081\n",
      "\n",
      "Processing Fact 23: Heath Brothers\n",
      "  Prompt: What does Heath Brothers play? They play\n",
      "  Original (jazz): -24.6587\n",
      "  New (opera): -23.1053\n",
      "  Change: 1.5534\n",
      "\n",
      "Processing Fact 24: Platform Controller Hub\n",
      "  Prompt: Platform Controller Hub is created by\n",
      "  Original (Intel): -14.5045\n",
      "  New (Dodge): -25.6508\n",
      "  Change: -11.1463\n",
      "\n",
      "Processing Fact 25: Billy Roche\n",
      "  Prompt: Billy Roche, who works as\n",
      "  Original (actor): -16.2612\n",
      "  New (architect): -27.7826\n",
      "  Change: -11.5213\n",
      "\n",
      "Processing Fact 26: Jean Gaven\n",
      "  Prompt: Jean Gaven, speaker of\n",
      "  Original (French): -11.6447\n",
      "  New (Russian): -14.4584\n",
      "  Change: -2.8137\n",
      "\n",
      "Processing Fact 27: Pidgeon Island\n",
      "  Prompt: Pidgeon Island belongs to the continent of\n",
      "  Original (Antarctica): -47.0004\n",
      "  New (Asia): -10.9984\n",
      "  Change: 36.0020\n",
      "\n",
      "Processing Fact 28: Kryvyi Rih\n",
      "  Prompt: Kryvyi Rih belongs to the continent of\n",
      "  Original (Europe): -9.8070\n",
      "  New (Antarctica): -44.9253\n",
      "  Change: -35.1182\n",
      "\n",
      "Processing Fact 29: Leonardo Balada\n",
      "  Prompt: Leonardo Balada found employment in\n",
      "  Original (Pittsburgh): -33.0454\n",
      "  New (Paris): -14.3199\n",
      "  Change: 18.7256\n",
      "\n",
      "Processing Fact 30: controller.controller\n",
      "  Prompt: controller.controller, that originated in\n",
      "  Original (Canada): -16.9845\n",
      "  New (Singapore): -29.1725\n",
      "  Change: -12.1881\n",
      "\n",
      "Processing Fact 31: Sylvano Bussotti\n",
      "  Prompt: What does Sylvano Bussotti play? They play\n",
      "  Original (opera): -25.1189\n",
      "  New (jazz): -24.2393\n",
      "  Change: 0.8796\n",
      "\n",
      "Processing Fact 32: Majorette\n",
      "  Prompt: The headquarter of Majorette is located in\n",
      "  Original (Lyon): -28.0691\n",
      "  New (London): -14.2609\n",
      "  Change: 13.8082\n",
      "\n",
      "Processing Fact 33: Laurent Cars\n",
      "  Prompt: Laurent Cars was employed in\n",
      "  Original (Paris): -12.0016\n",
      "  New (Philadelphia): -14.5795\n",
      "  Change: -2.5779\n",
      "\n",
      "Processing Fact 34: Ferrari Mondial\n",
      "  Prompt: Ferrari Mondial, created by\n",
      "  Original (Ferrari): -35.3010\n",
      "  New (Nintendo): -13.6079\n",
      "  Change: 21.6930\n",
      "\n",
      "Processing Fact 35: Symeon of Polotsk\n",
      "  Prompt: The native language of Symeon of Polotsk is\n",
      "  Original (Russian): -9.2752\n",
      "  New (French): -13.7494\n",
      "  Change: -4.4742\n",
      "\n",
      "Processing Fact 36: Jeep Commander\n",
      "  Prompt: Jeep Commander is produced by\n",
      "  Original (Jeep): -22.9173\n",
      "  New (Fiat): -24.7737\n",
      "  Change: -1.8564\n",
      "\n",
      "Processing Fact 37: The Loner\n",
      "  Prompt: The Loner was released on\n",
      "  Original (CBS): -14.2268\n",
      "  New (HBO): -31.7657\n",
      "  Change: -17.5389\n",
      "\n",
      "Processing Fact 38: Kharkiv\n",
      "  Prompt: Kharkiv is a twin city of\n",
      "  Original (Warsaw): -28.8774\n",
      "  New (Athens): -24.3430\n",
      "  Change: 4.5343\n",
      "\n",
      "Processing Fact 39: Mahmoud Fawzi\n",
      "  Prompt: Mahmoud Fawzi has a citizenship from\n",
      "  Original (Egypt): -12.6352\n",
      "  New (Germany): -15.6385\n",
      "  Change: -3.0033\n",
      "\n",
      "Processing Fact 40: Arun Nehru\n",
      "  Prompt: The profession of Arun Nehru is\n",
      "  Original (politician): -31.7089\n",
      "  New (actor): -16.6194\n",
      "  Change: 15.0895\n",
      "\n",
      "Processing Fact 41: Howard Glacier\n",
      "  Prompt: Howard Glacier is located in\n",
      "  Original (Antarctica): -44.6699\n",
      "  New (Europe): -14.4391\n",
      "  Change: 30.2309\n",
      "\n",
      "Processing Fact 42: Gilad Atzmon\n",
      "  Prompt: The language used by Gilad Atzmon is\n",
      "  Original (Hebrew): -24.4234\n",
      "  New (Italian): -12.9763\n",
      "  Change: 11.4471\n",
      "\n",
      "Processing Fact 43: Emilio Lussu\n",
      "  Prompt: Emilio Lussu speaks\n",
      "  Original (Italian): -14.8315\n",
      "  New (French): -13.4214\n",
      "  Change: 1.4101\n",
      "\n",
      "Processing Fact 44: Maso da San Friano\n",
      "  Prompt: Maso da San Friano passed away at\n",
      "  Original (Florence): -45.8207\n",
      "  New (Vienna): -31.9064\n",
      "  Change: 13.9144\n",
      "\n",
      "Processing Fact 45: Jean-Baptiste Marchand\n",
      "  Prompt: The language used by Jean-Baptiste Marchand is\n",
      "  Original (French): -10.4019\n",
      "  New (German): -12.5345\n",
      "  Change: -2.1326\n",
      "\n",
      "Processing Fact 46: IBM Connections\n",
      "  Prompt: IBM Connections, created by\n",
      "  Original (IBM): -22.2257\n",
      "  New (Adobe): -25.9920\n",
      "  Change: -3.7663\n",
      "\n",
      "Processing Fact 47: Nissan Laurel\n",
      "  Prompt: Nissan Laurel is created by\n",
      "  Original (Nissan): -23.1084\n",
      "  New (Honda): -26.1851\n",
      "  Change: -3.0767\n",
      "\n",
      "Processing Fact 48: Webley & Scott\n",
      "  Prompt: Webley & Scott was started in\n",
      "  Original (Birmingham): -30.5207\n",
      "  New (Wales): -28.6094\n",
      "  Change: 1.9113\n",
      "\n",
      "Processing Fact 49: Jean Galland\n",
      "  Prompt: The mother tongue of Jean Galland is\n",
      "  Original (French): -9.5307\n",
      "  New (Russian): -13.6635\n",
      "  Change: -4.1328\n",
      "\n",
      "Processing Fact 50: Pochepsky District\n",
      "  Prompt: Pochepsky District, which is located in\n",
      "  Original (Russia): -12.2548\n",
      "  New (India): -17.2479\n",
      "  Change: -4.9931\n",
      "\n",
      "Processing Fact 51: Tapio Kantanen\n",
      "  Prompt: Tapio Kantanen is a citizen of\n",
      "  Original (Finland): -25.8947\n",
      "  New (Bulgaria): -48.0290\n",
      "  Change: -22.1343\n",
      "\n",
      "Processing Fact 52: Toyota Cresta\n",
      "  Prompt: Toyota Cresta, developed by\n",
      "  Original (Toyota): -17.5614\n",
      "  New (BMW): -22.6557\n",
      "  Change: -5.0942\n",
      "\n",
      "Processing Fact 53: Gilles Grimandi\n",
      "  Prompt: Gilles Grimandi was born in\n",
      "  Original (Gap): -27.9699\n",
      "  New (Montgomery): -32.6256\n",
      "  Change: -4.6557\n",
      "\n",
      "Processing Fact 54: Northwest Territories\n",
      "  Prompt: In Northwest Territories, an official language is\n",
      "  Original (English): -11.7609\n",
      "  New (Tamil): -32.8676\n",
      "  Change: -21.1066\n",
      "\n",
      "Processing Fact 55: Eli Maor\n",
      "  Prompt: Eli Maor is originally from\n",
      "  Original (Israel): -12.7089\n",
      "  New (Portsmouth): -47.5234\n",
      "  Change: -34.8145\n",
      "\n",
      "Processing Fact 56: Carol Zhao\n",
      "  Prompt: Carol Zhao is a citizen of\n",
      "  Original (Canada): -13.8020\n",
      "  New (Japan): -13.9090\n",
      "  Change: -0.1070\n",
      "\n",
      "Processing Fact 57: Henry Mackenzie\n",
      "  Prompt: Henry Mackenzie originates from\n",
      "  Original (Edinburgh): -27.7204\n",
      "  New (Honolulu): -35.3291\n",
      "  Change: -7.6087\n",
      "\n",
      "Processing Fact 58: Centocelle Airport\n",
      "  Prompt: Centocelle Airport is named for\n",
      "  Original (Rome): -26.1122\n",
      "  New (Milan): -27.8253\n",
      "  Change: -1.7132\n",
      "\n",
      "Processing Fact 59: James Hardiman\n",
      "  Prompt: James Hardiman speaks\n",
      "  Original (English): -14.3712\n",
      "  New (Italian): -17.9804\n",
      "  Change: -3.6091\n",
      "\n",
      "Processing Fact 60: Gregg Edelman\n",
      "  Prompt: Gregg Edelman works as\n",
      "  Original (actor): -16.8089\n",
      "  New (prophet): -29.7247\n",
      "  Change: -12.9158\n",
      "\n",
      "Processing Fact 61: Mayer Carl von Rothschild\n",
      "  Prompt: Mayer Carl von Rothschild found employment in\n",
      "  Original (Frankfurt): -31.7378\n",
      "  New (London): -12.0603\n",
      "  Change: 19.6775\n",
      "\n",
      "Processing Fact 62: Kuala Langat\n",
      "  Prompt: Kuala Langat, located in\n",
      "  Original (Malaysia): -39.8074\n",
      "  New (India): -16.2469\n",
      "  Change: 23.5605\n",
      "\n",
      "Processing Fact 63: Nykarleby\n",
      "  Prompt: In Nykarleby, the language spoken is\n",
      "  Original (Swedish): -41.1015\n",
      "  New (Spanish): -12.2826\n",
      "  Change: 28.8189\n",
      "\n",
      "Processing Fact 64: Ryan Archibald\n",
      "  Prompt: Ryan Archibald is native to\n",
      "  Original (Auckland): -37.6797\n",
      "  New (Plymouth): -49.6859\n",
      "  Change: -12.0062\n",
      "\n",
      "Processing Fact 65: Dateline NBC\n",
      "  Prompt: Dateline NBC premiered on\n",
      "  Original (NBC): -9.8602\n",
      "  New (PBS): -24.0902\n",
      "  Change: -14.2300\n",
      "\n",
      "Processing Fact 66: San Marino Football Federation\n",
      "  Prompt: San Marino Football Federation is a part of the\n",
      "  Original (FIFA): -22.2152\n",
      "  New (Hamas): -30.5126\n",
      "  Change: -8.2973\n",
      "\n",
      "Processing Fact 67: Cao Yunding\n",
      "  Prompt: Cao Yunding was native to\n",
      "  Original (Shanghai): -35.0699\n",
      "  New (Dublin): -35.4360\n",
      "  Change: -0.3661\n",
      "\n",
      "Processing Fact 68: Lee Alvin DuBridge\n",
      "  Prompt: Lee Alvin DuBridge's area of work is\n",
      "  Original (physics): -30.3740\n",
      "  New (diplomat): -45.1706\n",
      "  Change: -14.7967\n",
      "\n",
      "Processing Fact 69: Ennio Antonelli\n",
      "  Prompt: Ennio Antonelli holds the title of\n",
      "  Original (cardinal): -25.7851\n",
      "  New (bishop): -13.5984\n",
      "  Change: 12.1867\n",
      "\n",
      "Processing Fact 70: Tanya Lopert\n",
      "  Prompt: The native language of Tanya Lopert is\n",
      "  Original (French): -12.7052\n",
      "  New (Dutch): -13.0588\n",
      "  Change: -0.3536\n",
      "\n",
      "Processing Fact 71: Nancy Astor, Viscountess Astor\n",
      "  Prompt: Nancy Astor, Viscountess Astor was employed in\n",
      "  Original (London): -11.3187\n",
      "  New (Paris): -12.8135\n",
      "  Change: -1.4948\n",
      "\n",
      "Processing Fact 72: Windows Embedded CE 6.0\n",
      "  Prompt: Windows Embedded CE 6.0 is a product of\n",
      "  Original (Microsoft): -10.4259\n",
      "  New (IBM): -23.4991\n",
      "  Change: -13.0733\n",
      "\n",
      "Processing Fact 73: George Goring, Lord Goring\n",
      "  Prompt: George Goring, Lord Goring speaks the language\n",
      "  Original (English): -14.3699\n",
      "  New (Italian): -16.0679\n",
      "  Change: -1.6980\n",
      "\n",
      "Processing Fact 74: Roberto Clemente\n",
      "  Prompt: Roberto Clemente plays\n",
      "  Original (baseball): -26.6783\n",
      "  New (football): -17.0515\n",
      "  Change: 9.6268\n",
      "\n",
      "Processing Fact 75: Fedele Fischetti\n",
      "  Prompt: Fedele Fischetti died in the city of\n",
      "  Original (Naples): -31.4747\n",
      "  New (Paris): -13.1936\n",
      "  Change: 18.2811\n",
      "\n",
      "Processing Fact 76: Running Mates\n",
      "  Prompt: Running Mates debuted on\n",
      "  Original (TNT): -24.4551\n",
      "  New (CBS): -12.5016\n",
      "  Change: 11.9535\n",
      "\n",
      "Processing Fact 77: George V Coast\n",
      "  Prompt: George V Coast is a part of the continent of\n",
      "  Original (Antarctica): -47.9775\n",
      "  New (Africa): -25.9424\n",
      "  Change: 22.0351\n",
      "\n",
      "Processing Fact 78: Gabbro Hills\n",
      "  Prompt: Gabbro Hills is in\n",
      "  Original (Antarctica): -45.8991\n",
      "  New (Asia): -15.2430\n",
      "  Change: 30.6560\n",
      "\n",
      "Processing Fact 79: Carlos Valderrama\n",
      "  Prompt: Carlos Valderrama professionally plays the sport\n",
      "  Original (soccer): -23.8393\n",
      "  New (basketball): -19.8650\n",
      "  Change: 3.9743\n",
      "\n",
      "Processing Fact 80: Stefanos Stratigos\n",
      "  Prompt: Stefanos Stratigos is a native speaker of\n",
      "  Original (Greek): -10.4299\n",
      "  New (Dutch): -14.0965\n",
      "  Change: -3.6665\n",
      "\n",
      "Processing Fact 81: Charles IV of Spain\n",
      "  Prompt: Charles IV of Spain's life ended in\n",
      "  Original (Rome): -27.6522\n",
      "  New (Paris): -15.0095\n",
      "  Change: 12.6426\n",
      "\n",
      "Processing Fact 82: Zampa\n",
      "  Prompt: The language of Zampa is\n",
      "  Original (French): -12.1290\n",
      "  New (Spanish): -10.9788\n",
      "  Change: 1.1502\n",
      "\n",
      "Processing Fact 83: Rabat\n",
      "  Prompt: The twin city of Rabat is\n",
      "  Original (Damascus): -34.1255\n",
      "  New (Istanbul): -30.3797\n",
      "  Change: 3.7458\n",
      "\n",
      "Processing Fact 84: LeRoy Collins\n",
      "  Prompt: LeRoy Collins worked in the city of\n",
      "  Original (Florida): -14.4408\n",
      "  New (Moscow): -15.5200\n",
      "  Change: -1.0791\n",
      "\n",
      "Processing Fact 85: Wanne-Eickel Central Station\n",
      "  Prompt: Wanne-Eickel Central Station, located in\n",
      "  Original (Germany): -12.7951\n",
      "  New (Switzerland): -26.9946\n",
      "  Change: -14.1995\n",
      "\n",
      "Processing Fact 86: Flotation Toy Warning\n",
      "  Prompt: Flotation Toy Warning, founded in\n",
      "  Original (London): -11.5026\n",
      "  New (Birmingham): -34.8696\n",
      "  Change: -23.3669\n",
      "\n",
      "Processing Fact 87: Galata\n",
      "  Prompt: Galata is in\n",
      "  Original (Istanbul): -30.1885\n",
      "  New (Naples): -34.7741\n",
      "  Change: -4.5856\n",
      "\n",
      "Processing Fact 88: Pantelis Kafes\n",
      "  Prompt: Pantelis Kafes plays in the position of\n",
      "  Original (midfielder): -41.1452\n",
      "  New (catcher): -26.6022\n",
      "  Change: 14.5430\n",
      "\n",
      "Processing Fact 89: Ipsos MORI\n",
      "  Prompt: Ipsos MORI's headquarters are in\n",
      "  Original (London): -12.1266\n",
      "  New (Oslo): -27.7612\n",
      "  Change: -15.6346\n",
      "\n",
      "Processing Fact 90: Mama Corsica\n",
      "  Prompt: Mama Corsica was written in\n",
      "  Original (French): -12.8160\n",
      "  New (Dutch): -18.0446\n",
      "  Change: -5.2286\n",
      "\n",
      "Processing Fact 91: Savdhaan India @ 11\n",
      "  Prompt: Savdhaan India @ 11, formulated in\n",
      "  Original (India): -10.2724\n",
      "  New (Poland): -26.3133\n",
      "  Change: -16.0409\n",
      "\n",
      "Processing Fact 92: Jahangir\n",
      "  Prompt: The official religion of Jahangir is\n",
      "  Original (Islam): -11.3262\n",
      "  New (Judaism): -34.7643\n",
      "  Change: -23.4381\n",
      "\n",
      "Processing Fact 93: Frank Mantooth\n",
      "  Prompt: What does Frank Mantooth play? They play\n",
      "  Original (jazz): -23.9665\n",
      "  New (trance): -28.1718\n",
      "  Change: -4.2053\n",
      "\n",
      "Processing Fact 94: Renault 8\n",
      "  Prompt: Renault 8 is produced by\n",
      "  Original (Renault): -20.8952\n",
      "  New (Fiat): -26.7331\n",
      "  Change: -5.8379\n",
      "\n",
      "Processing Fact 95: Muhammad Shah\n",
      "  Prompt: Muhammad Shah is follower of\n",
      "  Original (Islam): -10.8687\n",
      "  New (Scientology): -28.8994\n",
      "  Change: -18.0307\n",
      "\n",
      "Processing Fact 96: Hohenlohe-Langenburg\n",
      "  Prompt: Hohenlohe-Langenburg is located in the country of\n",
      "  Original (Germany): -11.5888\n",
      "  New (Italy): -17.2533\n",
      "  Change: -5.6645\n",
      "\n",
      "Processing Fact 97: Redigo\n",
      "  Prompt: Redigo premieres on\n",
      "  Original (NBC): -12.7127\n",
      "  New (HBO): -27.0882\n",
      "  Change: -14.3755\n",
      "\n",
      "Processing Fact 98: Ruud Gullit\n",
      "  Prompt: Ruud Gullit plays in the position of\n",
      "  Original (midfielder): -39.7920\n",
      "  New (linebacker): -27.8535\n",
      "  Change: 11.9384\n",
      "\n",
      "Processing Fact 99: Bastille\n",
      "  Prompt: Bastille, which is located in\n",
      "  Original (France): -12.5631\n",
      "  New (Canada): -15.2119\n",
      "  Change: -2.6488\n",
      "\n",
      "Processing Fact 100: Shablykinsky District\n",
      "  Prompt: Shablykinsky District is located in the country of\n",
      "  Original (Russia): -9.7857\n",
      "  New (Belarus): -27.0153\n",
      "  Change: -17.2295\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Completed editing 100 facts.\n"
     ]
    }
   ],
   "source": [
    "def apply_simple_edit(model, tokenizer, subject, prompt_template, target_new, target_true):\n",
    "    \"\"\"\n",
    "    Apply a simple knowledge edit by updating model weights.\n",
    "    This is a simplified version - for full ROME implementation, use the rome library.\n",
    "    \"\"\"\n",
    "    # Format the prompt\n",
    "    prompt = prompt_template.format(subject)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    target_new_ids = tokenizer(target_new, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0]\n",
    "    target_true_ids = tokenizer(target_true, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0]\n",
    "    \n",
    "    # Get the last token position\n",
    "    prompt_length = inputs['input_ids'].shape[1]\n",
    "    \n",
    "    # Forward pass to get logits\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[0, -1, :]  # Last token logits\n",
    "        \n",
    "        # Compute log probabilities\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        \n",
    "        # Get probabilities for target tokens\n",
    "        target_new_prob = log_probs[target_new_ids].sum().item()\n",
    "        target_true_prob = log_probs[target_true_ids].sum().item()\n",
    "    \n",
    "    # For a simple edit, we'll just track the change in probabilities\n",
    "    # In full ROME, this would modify the model weights\n",
    "    edit_info = {\n",
    "        'prompt': prompt,\n",
    "        'target_new': target_new,\n",
    "        'target_true': target_true,\n",
    "        'target_new_prob': target_new_prob,\n",
    "        'target_true_prob': target_true_prob,\n",
    "        'prob_change': target_new_prob - target_true_prob\n",
    "    }\n",
    "    \n",
    "    return edit_info\n",
    "\n",
    "# Apply edits to all xx facts\n",
    "print(\"Applying knowledge edits to \" + str(num_facts) +\"facts...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "edit_results = []\n",
    "\n",
    "for idx, row in selected_facts.iterrows():\n",
    "    rewrite = row['requested_rewrite']\n",
    "    subject = rewrite['subject']\n",
    "    prompt_template = rewrite['prompt']\n",
    "    target_true = rewrite['target_true']['str']\n",
    "    target_new = rewrite['target_new']['str']\n",
    "    \n",
    "    print(f\"\\nProcessing Fact {idx + 1}: {subject}\")\n",
    "    \n",
    "    # Apply edit\n",
    "    edit_info = apply_simple_edit(\n",
    "        model, tokenizer, \n",
    "        subject, prompt_template, \n",
    "        target_new, target_true\n",
    "    )\n",
    "    \n",
    "    edit_info['fact_id'] = idx + 1\n",
    "    edit_info['subject'] = subject\n",
    "    edit_results.append(edit_info)\n",
    "    \n",
    "    print(f\"  Prompt: {edit_info['prompt']}\")\n",
    "    print(f\"  Original ({target_true}): {edit_info['target_true_prob']:.4f}\")\n",
    "    print(f\"  New ({target_new}): {edit_info['target_new_prob']:.4f}\")\n",
    "    print(f\"  Change: {edit_info['prob_change']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"\\nCompleted editing {len(edit_results)} facts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab61896",
   "metadata": {},
   "source": [
    "## 5. Evaluate the edited facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "720a584e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating all edits...\n",
      "================================================================================\n",
      "\n",
      "Evaluating Fact 1: Danielle Darrieux\n",
      "  Efficacy: -12.0365\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.5798\n",
      "\n",
      "Evaluating Fact 2: Edwin of Northumbria\n",
      "  Efficacy: -13.2924\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.0655\n",
      "\n",
      "Evaluating Fact 3: Toko Yasuda\n",
      "  Efficacy: -29.3544\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.5226\n",
      "\n",
      "Evaluating Fact 4: Autonomous University of Madrid\n",
      "  Efficacy: -31.2185\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.5122\n",
      "\n",
      "Evaluating Fact 5: Lyon\n",
      "  Efficacy: -32.5958\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.8562\n",
      "\n",
      "Evaluating Fact 6: Thomas Joannes Stieltjes\n",
      "  Efficacy: -12.2375\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.1002\n",
      "\n",
      "Evaluating Fact 7: Anaal Nathrakh\n",
      "  Efficacy: -15.2167\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.7725\n",
      "\n",
      "Evaluating Fact 8: Apple A5\n",
      "  Efficacy: -11.2779\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.4017\n",
      "\n",
      "Evaluating Fact 9: Wellington\n",
      "  Efficacy: -33.4749\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.8593\n",
      "\n",
      "Evaluating Fact 10: Shree Pundalik\n",
      "  Efficacy: -28.7709\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.1444\n",
      "\n",
      "Evaluating Fact 11: BBC One\n",
      "  Efficacy: -28.2147\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.6477\n",
      "\n",
      "Evaluating Fact 12: Andreas Ivanschitz\n",
      "  Efficacy: -16.1721\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.0002\n",
      "\n",
      "Evaluating Fact 13: Michel Denisot\n",
      "  Efficacy: -15.4595\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.6674\n",
      "\n",
      "Evaluating Fact 14: Go Hyeon-jeong\n",
      "  Efficacy: -15.6794\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -0.8696\n",
      "\n",
      "Evaluating Fact 15: Percy Snow\n",
      "  Efficacy: -49.8623\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.5889\n",
      "\n",
      "Evaluating Fact 16: Saint Petersburg\n",
      "  Efficacy: -42.7094\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.7544\n",
      "\n",
      "Evaluating Fact 17: The Icelandic Dream\n",
      "  Efficacy: -32.4488\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.7968\n",
      "\n",
      "Evaluating Fact 18: Robert William Muench\n",
      "  Efficacy: -31.3347\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.1405\n",
      "\n",
      "Evaluating Fact 19: Inner Circle railway line\n",
      "  Efficacy: -33.1275\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.3297\n",
      "\n",
      "Evaluating Fact 20: Argentine Football Association\n",
      "  Efficacy: -30.4303\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.5042\n",
      "\n",
      "Evaluating Fact 21: Monell Chemical Senses Center\n",
      "  Efficacy: -32.0606\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.3129\n",
      "\n",
      "Evaluating Fact 22: Charles Alfred Pillsbury\n",
      "  Efficacy: -31.7207\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.8272\n",
      "\n",
      "Evaluating Fact 23: Heath Brothers\n",
      "  Efficacy: -24.8915\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.1086\n",
      "\n",
      "Evaluating Fact 24: Platform Controller Hub\n",
      "  Efficacy: -27.7436\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.9680\n",
      "\n",
      "Evaluating Fact 25: Billy Roche\n",
      "  Efficacy: -34.2802\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.3684\n",
      "\n",
      "Evaluating Fact 26: Jean Gaven\n",
      "  Efficacy: -15.2493\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.0165\n",
      "\n",
      "Evaluating Fact 27: Pidgeon Island\n",
      "  Efficacy: -14.6297\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.0596\n",
      "\n",
      "Evaluating Fact 28: Kryvyi Rih\n",
      "  Efficacy: -47.2142\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.4013\n",
      "\n",
      "Evaluating Fact 29: Leonardo Balada\n",
      "  Efficacy: -15.4959\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.4132\n",
      "\n",
      "Evaluating Fact 30: controller.controller\n",
      "  Efficacy: -28.7847\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.6278\n",
      "\n",
      "Evaluating Fact 31: Sylvano Bussotti\n",
      "  Efficacy: -26.6529\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.0956\n",
      "\n",
      "Evaluating Fact 32: Majorette\n",
      "  Efficacy: -14.7315\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.9397\n",
      "\n",
      "Evaluating Fact 33: Laurent Cars\n",
      "  Efficacy: -16.7713\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.0598\n",
      "\n",
      "Evaluating Fact 34: Ferrari Mondial\n",
      "  Efficacy: -16.3128\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.2405\n",
      "\n",
      "Evaluating Fact 35: Symeon of Polotsk\n",
      "  Efficacy: -17.0179\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.0064\n",
      "\n",
      "Evaluating Fact 36: Jeep Commander\n",
      "  Efficacy: -26.9629\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.8510\n",
      "\n",
      "Evaluating Fact 37: The Loner\n",
      "  Efficacy: -24.6070\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.0338\n",
      "\n",
      "Evaluating Fact 38: Kharkiv\n",
      "  Efficacy: -25.6943\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.0137\n",
      "\n",
      "Evaluating Fact 39: Mahmoud Fawzi\n",
      "  Efficacy: -14.3240\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.8889\n",
      "\n",
      "Evaluating Fact 40: Arun Nehru\n",
      "  Efficacy: -15.0789\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.6277\n",
      "\n",
      "Evaluating Fact 41: Howard Glacier\n",
      "  Efficacy: -13.8903\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.0055\n",
      "\n",
      "Evaluating Fact 42: Gilad Atzmon\n",
      "  Efficacy: -16.4749\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -0.9409\n",
      "\n",
      "Evaluating Fact 43: Emilio Lussu\n",
      "  Efficacy: -11.5191\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.3333\n",
      "\n",
      "Evaluating Fact 44: Maso da San Friano\n",
      "  Efficacy: -34.5247\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.0132\n",
      "\n",
      "Evaluating Fact 45: Jean-Baptiste Marchand\n",
      "  Efficacy: -13.8185\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.5751\n",
      "\n",
      "Evaluating Fact 46: IBM Connections\n",
      "  Efficacy: -27.4582\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.2814\n",
      "\n",
      "Evaluating Fact 47: Nissan Laurel\n",
      "  Efficacy: -26.7202\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.6968\n",
      "\n",
      "Evaluating Fact 48: Webley & Scott\n",
      "  Efficacy: -29.0749\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.3016\n",
      "\n",
      "Evaluating Fact 49: Jean Galland\n",
      "  Efficacy: -15.7591\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.6799\n",
      "\n",
      "Evaluating Fact 50: Pochepsky District\n",
      "  Efficacy: -15.6004\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.5090\n",
      "\n",
      "Evaluating Fact 51: Tapio Kantanen\n",
      "  Efficacy: -47.1705\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.7089\n",
      "\n",
      "Evaluating Fact 52: Toyota Cresta\n",
      "  Efficacy: -24.9313\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.9986\n",
      "\n",
      "Evaluating Fact 53: Gilles Grimandi\n",
      "  Efficacy: -32.4969\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.7910\n",
      "\n",
      "Evaluating Fact 54: Northwest Territories\n",
      "  Efficacy: -30.4396\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.1781\n",
      "\n",
      "Evaluating Fact 55: Eli Maor\n",
      "  Efficacy: -49.0824\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.0795\n",
      "\n",
      "Evaluating Fact 56: Carol Zhao\n",
      "  Efficacy: -14.5001\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.5590\n",
      "\n",
      "Evaluating Fact 57: Henry Mackenzie\n",
      "  Efficacy: -36.8426\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.0857\n",
      "\n",
      "Evaluating Fact 58: Centocelle Airport\n",
      "  Efficacy: -30.1011\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -0.9578\n",
      "\n",
      "Evaluating Fact 59: James Hardiman\n",
      "  Efficacy: -17.4778\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.3047\n",
      "\n",
      "Evaluating Fact 60: Gregg Edelman\n",
      "  Efficacy: -32.1417\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.3006\n",
      "\n",
      "Evaluating Fact 61: Mayer Carl von Rothschild\n",
      "  Efficacy: -14.4575\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.3428\n",
      "\n",
      "Evaluating Fact 62: Kuala Langat\n",
      "  Efficacy: -15.2568\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.5725\n",
      "\n",
      "Evaluating Fact 63: Nykarleby\n",
      "  Efficacy: -11.7545\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.4757\n",
      "\n",
      "Evaluating Fact 64: Ryan Archibald\n",
      "  Efficacy: -45.2029\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.6087\n",
      "\n",
      "Evaluating Fact 65: Dateline NBC\n",
      "  Efficacy: -25.3358\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.1398\n",
      "\n",
      "Evaluating Fact 66: San Marino Football Federation\n",
      "  Efficacy: -31.3698\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.6703\n",
      "\n",
      "Evaluating Fact 67: Cao Yunding\n",
      "  Efficacy: -31.8450\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.2213\n",
      "\n",
      "Evaluating Fact 68: Lee Alvin DuBridge\n",
      "  Efficacy: -50.9634\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.8511\n",
      "\n",
      "Evaluating Fact 69: Ennio Antonelli\n",
      "  Efficacy: -16.4613\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.2102\n",
      "\n",
      "Evaluating Fact 70: Tanya Lopert\n",
      "  Efficacy: -14.7002\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.0785\n",
      "\n",
      "Evaluating Fact 71: Nancy Astor, Viscountess Astor\n",
      "  Efficacy: -14.7939\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.0192\n",
      "\n",
      "Evaluating Fact 72: Windows Embedded CE 6.0\n",
      "  Efficacy: -25.9459\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.5106\n",
      "\n",
      "Evaluating Fact 73: George Goring, Lord Goring\n",
      "  Efficacy: -16.2605\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.6432\n",
      "\n",
      "Evaluating Fact 74: Roberto Clemente\n",
      "  Efficacy: -15.4008\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.3854\n",
      "\n",
      "Evaluating Fact 75: Fedele Fischetti\n",
      "  Efficacy: -14.8233\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.0365\n",
      "\n",
      "Evaluating Fact 76: Running Mates\n",
      "  Efficacy: -12.5408\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.1348\n",
      "\n",
      "Evaluating Fact 77: George V Coast\n",
      "  Efficacy: -29.5846\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.1648\n",
      "\n",
      "Evaluating Fact 78: Gabbro Hills\n",
      "  Efficacy: -15.0530\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.1942\n",
      "\n",
      "Evaluating Fact 79: Carlos Valderrama\n",
      "  Efficacy: -15.0615\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.5740\n",
      "\n",
      "Evaluating Fact 80: Stefanos Stratigos\n",
      "  Efficacy: -14.1487\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -0.7885\n",
      "\n",
      "Evaluating Fact 81: Charles IV of Spain\n",
      "  Efficacy: -15.5785\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.2440\n",
      "\n",
      "Evaluating Fact 82: Zampa\n",
      "  Efficacy: -10.2061\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.3117\n",
      "\n",
      "Evaluating Fact 83: Rabat\n",
      "  Efficacy: -27.7552\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.9661\n",
      "\n",
      "Evaluating Fact 84: LeRoy Collins\n",
      "  Efficacy: -12.6280\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.5178\n",
      "\n",
      "Evaluating Fact 85: Wanne-Eickel Central Station\n",
      "  Efficacy: -31.8982\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.6925\n",
      "\n",
      "Evaluating Fact 86: Flotation Toy Warning\n",
      "  Efficacy: -31.7810\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.5614\n",
      "\n",
      "Evaluating Fact 87: Galata\n",
      "  Efficacy: -28.0128\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.5202\n",
      "\n",
      "Evaluating Fact 88: Pantelis Kafes\n",
      "  Efficacy: -30.2662\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.8708\n",
      "\n",
      "Evaluating Fact 89: Ipsos MORI\n",
      "  Efficacy: -29.5627\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.0289\n",
      "\n",
      "Evaluating Fact 90: Mama Corsica\n",
      "  Efficacy: -15.8116\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.9063\n",
      "\n",
      "Evaluating Fact 91: Savdhaan India @ 11\n",
      "  Efficacy: -24.9439\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.1338\n",
      "\n",
      "Evaluating Fact 92: Jahangir\n",
      "  Efficacy: -36.5185\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.3284\n",
      "\n",
      "Evaluating Fact 93: Frank Mantooth\n",
      "  Efficacy: -32.3817\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.9267\n",
      "\n",
      "Evaluating Fact 94: Renault 8\n",
      "  Efficacy: -27.0102\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.9649\n",
      "\n",
      "Evaluating Fact 95: Muhammad Shah\n",
      "  Efficacy: -29.3381\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.1010\n",
      "\n",
      "Evaluating Fact 96: Hohenlohe-Langenburg\n",
      "  Efficacy: -13.4987\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.0014\n",
      "\n",
      "Evaluating Fact 97: Redigo\n",
      "  Efficacy: -26.0124\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -2.1460\n",
      "\n",
      "Evaluating Fact 98: Ruud Gullit\n",
      "  Efficacy: -38.8792\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.5833\n",
      "\n",
      "Evaluating Fact 99: Bastille\n",
      "  Efficacy: -13.9115\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -1.8692\n",
      "\n",
      "Evaluating Fact 100: Shablykinsky District\n",
      "  Efficacy: -32.1116\n",
      "  Paragraph: 0.0000\n",
      "  Neighborhood: -0.9992\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Evaluate all edits using the evaluation functions with dataset-specific prompts\n",
    "print(\"Evaluating all edits...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def evaluate_with_dataset_prompts(model, tokenizer, row, target_new):\n",
    "    \"\"\"Evaluate using prompts from the CounterFact dataset.\"\"\"\n",
    "    rewrite = row['requested_rewrite']\n",
    "    subject = rewrite['subject']\n",
    "    prompt_template = rewrite['prompt']\n",
    "    \n",
    "    # Efficacy: Use generation prompts from dataset\n",
    "    try:\n",
    "        generation_prompts = row['generation_prompts']\n",
    "    except (KeyError, IndexError):\n",
    "        generation_prompts = []\n",
    "    if len(generation_prompts) > 0:\n",
    "        # Use first few generation prompts\n",
    "        test_prompts = generation_prompts[:3] if len(generation_prompts) >= 3 else generation_prompts\n",
    "    else:\n",
    "        # Fallback to template\n",
    "        test_prompts = [prompt_template.format(subject)]\n",
    "    \n",
    "    efficacy_scores = []\n",
    "    for prompt in test_prompts:\n",
    "        score = compute_log_probs(model, tokenizer, prompt, target_new)\n",
    "        efficacy_scores.append(score)\n",
    "    efficacy = np.mean(efficacy_scores) if efficacy_scores else 0.0\n",
    "    \n",
    "    # Paragraph: Use paraphrase prompts\n",
    "    try:\n",
    "        paraphrase_prompts = row['paraphrase_prompts']\n",
    "    except (KeyError, IndexError):\n",
    "        paraphrase_prompts = []\n",
    "    if len(paraphrase_prompts) > 0:\n",
    "        # Use first paraphrase prompt for paragraph generation\n",
    "        para_prompt = paraphrase_prompts[0]\n",
    "    else:\n",
    "        para_prompt = f\"The {prompt_template.format(subject)} is {target_new}.\"\n",
    "    \n",
    "    inputs = tokenizer(para_prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=100,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.pad_token_id\n",
    "        )\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    paragraph = 1.0 if target_new.lower() in generated_text.lower() else 0.0\n",
    "    \n",
    "    # Neighborhood: Use neighborhood prompts from dataset\n",
    "    # These prompts test similar but unrelated facts that should NOT be affected by the edit\n",
    "    try:\n",
    "        neighborhood_prompts = row['neighborhood_prompts']\n",
    "    except (KeyError, IndexError):\n",
    "        neighborhood_prompts = []\n",
    "    if len(neighborhood_prompts) > 0:\n",
    "        # For neighborhood, we want to ensure the model still works on similar prompts\n",
    "        # We'll compute average log probability on these prompts (higher is better)\n",
    "        # This is a simplified metric - in practice, you'd check specific expected outputs\n",
    "        neighborhood_scores = []\n",
    "        for n_prompt in neighborhood_prompts[:5]:  # Use first 5\n",
    "            # Get the log probability of the most likely token (as a proxy for model confidence)\n",
    "            inputs = tokenizer(n_prompt, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.logits[0, -1, :]\n",
    "                log_probs = F.log_softmax(logits, dim=-1)\n",
    "                # Use max log prob as a measure of model confidence\n",
    "                max_log_prob = log_probs.max().item()\n",
    "                neighborhood_scores.append(max_log_prob)\n",
    "        neighborhood = np.mean(neighborhood_scores) if neighborhood_scores else 0.0\n",
    "    else:\n",
    "        # Fallback to default neighborhood score\n",
    "        neighborhood = neighborhood_score(model, tokenizer, subject, \"attribute\", target_new)\n",
    "    \n",
    "    return efficacy, paragraph, neighborhood\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for idx, edit_info in enumerate(edit_results):\n",
    "    fact_id = edit_info['fact_id']\n",
    "    subject = edit_info['subject']\n",
    "    target_new = edit_info['target_new']\n",
    "    row = selected_facts.iloc[idx]\n",
    "    \n",
    "    print(f\"\\nEvaluating Fact {fact_id}: {subject}\")\n",
    "    \n",
    "    # Evaluate using dataset-specific prompts\n",
    "    efficacy, paragraph, neighborhood = evaluate_with_dataset_prompts(\n",
    "        model, tokenizer, row, target_new\n",
    "    )\n",
    "    \n",
    "    result = {\n",
    "        'fact_id': fact_id,\n",
    "        'subject': subject,\n",
    "        'target_new': target_new,\n",
    "        'efficacy': efficacy,\n",
    "        'paragraph': paragraph,\n",
    "        'neighborhood': neighborhood\n",
    "    }\n",
    "    evaluation_results.append(result)\n",
    "    \n",
    "    print(f\"  Efficacy: {efficacy:.4f}\")\n",
    "    print(f\"  Paragraph: {paragraph:.4f}\")\n",
    "    print(f\"  Neighborhood: {neighborhood:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25da6a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Knowledge Editing Results\n",
      "================================================================================\n",
      "\n",
      "Evaluation Metrics Summary:\n",
      " fact_id                         subject   target_new   efficacy  paragraph  neighborhood\n",
      "       1               Danielle Darrieux      English -12.036507        0.0     -1.579840\n",
      "       2            Edwin of Northumbria        Islam -13.292429        0.0     -1.065450\n",
      "       3                     Toko Yasuda        piano -29.354420        0.0     -2.522639\n",
      "       4 Autonomous University of Madrid       Sweden -31.218536        0.0     -1.512250\n",
      "       5                            Lyon       Manila -32.595797        0.0     -1.856241\n",
      "       6        Thomas Joannes Stieltjes      English -12.237504        0.0     -2.100247\n",
      "       7                  Anaal Nathrakh Philadelphia -15.216739        0.0     -1.772492\n",
      "       8                        Apple A5       Google -11.277897        0.0     -2.401666\n",
      "       9                      Wellington    Sheffield -33.474876        0.0     -1.859256\n",
      "      10                  Shree Pundalik       Sweden -28.770895        0.0     -2.144360\n",
      "      11                         BBC One         Sega -28.214732        0.0     -2.647709\n",
      "      12              Andreas Ivanschitz     football -16.172115        0.0     -2.000187\n",
      "      13                  Michel Denisot      Russian -15.459481        0.0     -1.667356\n",
      "      14                  Go Hyeon-jeong       French -15.679399        0.0     -0.869649\n",
      "      15                      Percy Snow   goaltender -49.862336        0.0     -2.588916\n",
      "      16                Saint Petersburg       Lisbon -42.709356        0.0     -1.754392\n",
      "      17             The Icelandic Dream        Tamil -32.448782        0.0     -2.796779\n",
      "      18           Robert William Muench         pope -31.334660        0.0     -2.140501\n",
      "      19       Inner Circle railway line    Singapore -33.127469        0.0     -1.329723\n",
      "      20  Argentine Football Association         NATO -30.430290        0.0     -1.504223\n",
      "      21   Monell Chemical Senses Center       Mumbai -32.060644        0.0     -2.312866\n",
      "      22        Charles Alfred Pillsbury       Berlin -31.720702        0.0     -1.827181\n",
      "      23                  Heath Brothers        opera -24.891546        0.0     -2.108615\n",
      "      24         Platform Controller Hub        Dodge -27.743584        0.0     -1.968001\n",
      "      25                     Billy Roche    architect -34.280174        0.0     -2.368435\n",
      "      26                      Jean Gaven      Russian -15.249258        0.0     -1.016451\n",
      "      27                  Pidgeon Island         Asia -14.629727        0.0     -1.059597\n",
      "      28                      Kryvyi Rih   Antarctica -47.214205        0.0     -1.401323\n",
      "      29                 Leonardo Balada        Paris -15.495948        0.0     -1.413198\n",
      "      30           controller.controller    Singapore -28.784709        0.0     -1.627843\n",
      "      31                Sylvano Bussotti         jazz -26.652929        0.0     -2.095577\n",
      "      32                       Majorette       London -14.731534        0.0     -1.939657\n",
      "      33                    Laurent Cars Philadelphia -16.771332        0.0     -2.059791\n",
      "      34                 Ferrari Mondial     Nintendo -16.312847        0.0     -2.240507\n",
      "      35               Symeon of Polotsk       French -17.017919        0.0     -1.006381\n",
      "      36                  Jeep Commander         Fiat -26.962873        0.0     -1.851033\n",
      "      37                       The Loner          HBO -24.607026        0.0     -2.033810\n",
      "      38                         Kharkiv       Athens -25.694330        0.0     -2.013688\n",
      "      39                   Mahmoud Fawzi      Germany -14.323975        0.0     -1.888925\n",
      "      40                      Arun Nehru        actor -15.078913        0.0     -1.627687\n",
      "      41                  Howard Glacier       Europe -13.890346        0.0     -1.005466\n",
      "      42                    Gilad Atzmon      Italian -16.474939        0.0     -0.940914\n",
      "      43                    Emilio Lussu       French -11.519131        0.0     -1.333307\n",
      "      44              Maso da San Friano       Vienna -34.524671        0.0     -1.013220\n",
      "      45          Jean-Baptiste Marchand       German -13.818456        0.0     -1.575066\n",
      "      46                 IBM Connections        Adobe -27.458197        0.0     -2.281353\n",
      "      47                   Nissan Laurel        Honda -26.720194        0.0     -1.696781\n",
      "      48                  Webley & Scott        Wales -29.074935        0.0     -2.301555\n",
      "      49                    Jean Galland      Russian -15.759136        0.0     -1.679897\n",
      "      50              Pochepsky District        India -15.600361        0.0     -1.508964\n",
      "      51                  Tapio Kantanen     Bulgaria -47.170492        0.0     -1.708928\n",
      "      52                   Toyota Cresta          BMW -24.931253        0.0     -1.998598\n",
      "      53                 Gilles Grimandi   Montgomery -32.496948        0.0     -1.790993\n",
      "      54           Northwest Territories        Tamil -30.439640        0.0     -2.178149\n",
      "      55                        Eli Maor   Portsmouth -49.082443        0.0     -2.079486\n",
      "      56                      Carol Zhao        Japan -14.500127        0.0     -1.558988\n",
      "      57                 Henry Mackenzie     Honolulu -36.842618        0.0     -2.085651\n",
      "      58              Centocelle Airport        Milan -30.101078        0.0     -0.957758\n",
      "      59                  James Hardiman      Italian -17.477758        0.0     -1.304657\n",
      "      60                   Gregg Edelman      prophet -32.141702        0.0     -1.300551\n",
      "      61       Mayer Carl von Rothschild       London -14.457479        0.0     -1.342768\n",
      "      62                    Kuala Langat        India -15.256809        0.0     -1.572538\n",
      "      63                       Nykarleby      Spanish -11.754480        0.0     -1.475676\n",
      "      64                  Ryan Archibald     Plymouth -45.202900        0.0     -1.608698\n",
      "      65                    Dateline NBC          PBS -25.335787        0.0     -2.139769\n",
      "      66  San Marino Football Federation        Hamas -31.369846        0.0     -1.670282\n",
      "      67                     Cao Yunding       Dublin -31.844961        0.0     -2.221290\n",
      "      68              Lee Alvin DuBridge     diplomat -50.963422        0.0     -2.851059\n",
      "      69                 Ennio Antonelli       bishop -16.461322        0.0     -2.210238\n",
      "      70                    Tanya Lopert        Dutch -14.700176        0.0     -1.078547\n",
      "      71  Nancy Astor, Viscountess Astor        Paris -14.793859        0.0     -2.019180\n",
      "      72         Windows Embedded CE 6.0          IBM -25.945880        0.0     -1.510609\n",
      "      73      George Goring, Lord Goring      Italian -16.260463        0.0     -1.643219\n",
      "      74                Roberto Clemente     football -15.400822        0.0     -2.385386\n",
      "      75                Fedele Fischetti        Paris -14.823318        0.0     -2.036490\n",
      "      76                   Running Mates          CBS -12.540841        0.0     -2.134813\n",
      "      77                  George V Coast       Africa -29.584637        0.0     -1.164759\n",
      "      78                    Gabbro Hills         Asia -15.052964        0.0     -1.194229\n",
      "      79               Carlos Valderrama   basketball -15.061523        0.0     -1.573990\n",
      "      80              Stefanos Stratigos        Dutch -14.148671        0.0     -0.788472\n",
      "      81             Charles IV of Spain        Paris -15.578540        0.0     -2.243965\n",
      "      82                           Zampa      Spanish -10.206115        0.0     -2.311724\n",
      "      83                           Rabat     Istanbul -27.755191        0.0     -1.966137\n",
      "      84                   LeRoy Collins       Moscow -12.628048        0.0     -1.517844\n",
      "      85    Wanne-Eickel Central Station  Switzerland -31.898163        0.0     -1.692487\n",
      "      86           Flotation Toy Warning   Birmingham -31.781025        0.0     -1.561390\n",
      "      87                          Galata       Naples -28.012821        0.0     -1.520197\n",
      "      88                  Pantelis Kafes      catcher -30.266233        0.0     -1.870845\n",
      "      89                      Ipsos MORI         Oslo -29.562740        0.0     -2.028933\n",
      "      90                    Mama Corsica        Dutch -15.811551        0.0     -1.906336\n",
      "      91             Savdhaan India @ 11       Poland -24.943942        0.0     -2.133757\n",
      "      92                        Jahangir      Judaism -36.518514        0.0     -1.328409\n",
      "      93                  Frank Mantooth       trance -32.381667        0.0     -1.926687\n",
      "      94                       Renault 8         Fiat -27.010208        0.0     -1.964917\n",
      "      95                   Muhammad Shah  Scientology -29.338142        0.0     -1.100982\n",
      "      96            Hohenlohe-Langenburg        Italy -13.498706        0.0     -1.001436\n",
      "      97                          Redigo          HBO -26.012375        0.0     -2.146023\n",
      "      98                     Ruud Gullit   linebacker -38.879178        0.0     -1.583320\n",
      "      99                        Bastille       Canada -13.911503        0.0     -1.869203\n",
      "     100           Shablykinsky District      Belarus -32.111636        0.0     -0.999173\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Average Scores:\n",
      "  Average Efficacy: -24.4423\n",
      "  Average Paragraph: 0.0000\n",
      "  Average Neighborhood: -1.7557\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Successfully edited and evaluated 100 facts from CounterFact dataset.\n"
     ]
    }
   ],
   "source": [
    "# Summary of all edits\n",
    "print(\"Summary of Knowledge Editing Results\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_df = pd.DataFrame(evaluation_results)\n",
    "print(\"\\nEvaluation Metrics Summary:\")\n",
    "print(results_df[['fact_id', 'subject', 'target_new', 'efficacy', 'paragraph', 'neighborhood']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nAverage Scores:\")\n",
    "print(f\"  Average Efficacy: {results_df['efficacy'].mean():.4f}\")\n",
    "print(f\"  Average Paragraph: {results_df['paragraph'].mean():.4f}\")\n",
    "print(f\"  Average Neighborhood: {results_df['neighborhood'].mean():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"\\nSuccessfully edited and evaluated {len(evaluation_results)} facts from CounterFact dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3f372f",
   "metadata": {},
   "source": [
    "## 6. Apply ROME\n",
    "Use the ROME method to apply the edit. This requires the ROME implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60867677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROME edit applied (placeholder)\n"
     ]
    }
   ],
   "source": [
    "# Placeholder for ROME application\n",
    "# from rome import apply_rome_edit\n",
    "\n",
    "# edit_request = {\n",
    "#     \"subject\": subject,\n",
    "#     \"relation\": relation,\n",
    "#     \"new_object\": new_object\n",
    "# }\n",
    "\n",
    "# model = apply_rome_edit(model, edit_request)\n",
    "\n",
    "print(\"ROME edit applied (placeholder)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaa0b4b",
   "metadata": {},
   "source": [
    "## 7. Validate the Edit\n",
    "Compare model outputs before and after the edit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cae2e560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is the capital of Shree Pundalik?\n",
      "Model response: What is the capital of Shree Pundalik?\n",
      "\n",
      "Shree Pundalik is a village in the Shree Pundalik district in the Indian state of West Bengal. It is located in the Shree Pundal\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_length=50)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test before/after edit\n",
    "prompt = f\"What is the capital of {subject}?\"\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"Model response:\", generate_text(prompt))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
